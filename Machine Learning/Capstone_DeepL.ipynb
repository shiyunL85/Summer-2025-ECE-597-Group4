{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr26Cq9_jE-m",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "GsxBWiGljNsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/combined_featured_dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GmH0tohmjPHO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['Clean_Message'].astype(str).tolist()\n",
        "labels = df['label'].values"
      ],
      "metadata": {
        "id": "8cvdocOoj1NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_NUM_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "EMBEDDING_DIM = 100"
      ],
      "metadata": {
        "id": "Bpsb15Irj1oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, lower=True, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Found %s unique tokens.\" % len(word_index))"
      ],
      "metadata": {
        "id": "nlyMbRpAj7Tm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y = np.array(labels)"
      ],
      "metadata": {
        "id": "xqtIwRpGj-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "yBh3GxDuj_xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "t3T8Za3NkCec",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=128,\n",
        "    epochs=5,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "tfh9xtsKkDuH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)"
      ],
      "metadata": {
        "id": "L5odZ3yokFXT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report (CNN):\\n\")\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "VlPG1BaMkHMv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Phish'], yticklabels=['Benign', 'Phish'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix (CNN)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "60Jbx3Oike0Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Training and Validation Accuracy (CNN)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Training and Validation Loss (CNN)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GlyFt3GkksQs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "model_cnn_lstm = tf.keras.Sequential([\n",
        "    Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Reshape((1, 128)),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn_lstm.compile(loss='binary_crossentropy',\n",
        "                       optimizer='adam',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "model_cnn_lstm.summary()"
      ],
      "metadata": {
        "id": "F8wFwVrrpAfC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_cnn_lstm = model_cnn_lstm.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=128,\n",
        "    epochs=5,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "OckC1QunpB1P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_prob_cnn_lstm = model_cnn_lstm.predict(X_test)\n",
        "y_pred_cnn_lstm = (y_pred_prob_cnn_lstm >= 0.5).astype(int)"
      ],
      "metadata": {
        "id": "3lmcJXpqpDV5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report (CNN + LSTM):\\n\")\n",
        "print(classification_report(y_test, y_pred_cnn_lstm, digits=4))"
      ],
      "metadata": {
        "id": "2CP33oh9pExA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred_cnn_lstm)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Benign', 'Phish'], yticklabels=['Benign', 'Phish'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix (CNN + LSTM)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q0NIL6AkpFxl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_cnn_lstm.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history_cnn_lstm.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Training and Validation Accuracy (CNN + LSTM)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history_cnn_lstm.history['loss'], label='Train Loss')\n",
        "plt.plot(history_cnn_lstm.history['val_loss'], label='Val Loss')\n",
        "plt.title('Training and Validation Loss (CNN + LSTM)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BuNpJGm0pG6w",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "EUkLDyVdpIRH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "y2tCTOKjppxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/combined_featured_dataset.csv')\n",
        "df = df[['Clean_Message', 'label']].dropna()\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "texts = df['Clean_Message'].tolist()\n",
        "labels = df['label'].tolist()"
      ],
      "metadata": {
        "id": "c7ASSX0qpqd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.1, stratify=labels, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "xElsOvjMpsXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "LIndd24Gptqn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
      ],
      "metadata": {
        "id": "5ZlxTH-BpvBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmailDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = EmailDataset(train_encodings, train_labels)\n",
        "test_dataset = EmailDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "bx5bPLuBpwP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
      ],
      "metadata": {
        "id": "3BJUcVAwp0rS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        ")"
      ],
      "metadata": {
        "id": "oKswX6oep2lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "Y-OeHrA3p6Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "1vJQFUpXp7kc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output = trainer.predict(test_dataset)\n",
        "preds = np.argmax(preds_output.predictions, axis=1)"
      ],
      "metadata": {
        "id": "VPukuBdFp843",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report (BERT):\\n\")\n",
        "print(classification_report(test_labels, preds, digits=4))"
      ],
      "metadata": {
        "id": "-z8gbCqaWz-n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(test_labels, preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Phish'], yticklabels=['Benign', 'Phish'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix (BERT)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q_78XJw9W1hF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "cnn_acc = accuracy_score(y_test, y_pred)\n",
        "cnn_prec = precision_score(y_test, y_pred)\n",
        "cnn_rec = recall_score(y_test, y_pred)\n",
        "cnn_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "cnn_lstm_acc = accuracy_score(y_test, y_pred_cnn_lstm)\n",
        "cnn_lstm_prec = precision_score(y_test, y_pred_cnn_lstm)\n",
        "cnn_lstm_rec = recall_score(y_test, y_pred_cnn_lstm)\n",
        "cnn_lstm_f1 = f1_score(y_test, y_pred_cnn_lstm)\n",
        "\n",
        "bert_acc = accuracy_score(test_labels, preds)\n",
        "bert_prec = precision_score(test_labels, preds)\n",
        "bert_rec = recall_score(test_labels, preds)\n",
        "bert_f1 = f1_score(test_labels, preds)"
      ],
      "metadata": {
        "id": "YwkSpT1aa90f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Model\": [\"CNN\", \"CNN + LSTM\", \"BERT\"],\n",
        "    \"Accuracy\": [cnn_acc, cnn_lstm_acc, bert_acc],\n",
        "    \"Precision\": [cnn_prec, cnn_lstm_prec, bert_prec],\n",
        "    \"Recall\": [cnn_rec, cnn_lstm_rec, bert_rec],\n",
        "    \"F1-score\": [cnn_f1, cnn_lstm_f1, bert_f1],\n",
        "}\n",
        "\n",
        "df_metrics = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "VNm59P4PbAZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics = df_metrics.round(4)\n",
        "df_metrics"
      ],
      "metadata": {
        "id": "H5x4sBcSbCck",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}